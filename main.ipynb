{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "raI2_VQLKhvE"
   },
   "source": [
    "### AC109B Final Project\n",
    "TEAM 37: Ellie Jungmin Han, Timothy Lee, Chih-Kang Chang, Dabin Choe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estimating Annual Radiation Intensities on Buildings using 3D Convolutional Neurel Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  The use of artificial intelligence and machine learning are increasing in prominence in the building, architecture and construction sectors (Kalogirou, 2000). One area that is ideally suited to take advantage of these powerful new technologies is building energy simulation and environmental analysis (Goldstein & Coco, 2015). \n",
    "\n",
    "  For the estimation of the energy flow and the annual radiation intensities on buildings, physics-based models are often used. The algorithms employed are fairly complicated, involving the solution of complex differential equations. The software for this simulation usually requires high computational power and a considerable amount of time to output accurate predictions. Therefore, data-driven models for predicting physical properties on buildings are becoming increasingly popular. Artificial Neural Network (ANN) analysis based on prefabricated or simulated data of environmental systems and is therefore likely to be better and appropriate for designers than other methods. \n",
    "\n",
    "There are increasing attention and publications in machine learning research predicting surface solar radiation (Yadav & Chandel (2014), Mohandes, Rehman, & Halawani (1998), Voyant et al., (2017)) and building energy prediction (Amasyali & El-Gohary, (2018), Goldstein & Coco, (2018)). ANN models may be used to provide innovative ways of solving design problems which allow designers to get instantaneous feedback on the effect of a proposed change in building design. \n",
    "\n",
    "**The objective of this project is to introduce deep learning methods to represent physical properties on buildings (annual radiation intensity and exposure) without learning physics-based models.** This will show the future capacity of integrated ANNs as a tool in building performance simulation and modeling.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Literature Review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- *Review of BPS problems*\n",
    "\n",
    "  Building performance simulation (BPS) program is a powerful tool that can be used to evaluate the energy performance and environmental impacts of buildings throughout their lifecycle. Environmental analysis (EA) and building geometry modeling (BGM), which form a major part of BPS, help designers make responsive and environmentally conscious designs in the early modeling stage (Radford and Gero, 1987). Environmental analysis includes PV calculator, daylight simulator, and weather analysis platforms while building surface modelings include parametric geometry calculations.\n",
    " \n",
    "  Since both EA and BGM are required for an efficient calculation of BPS, many researchers attempted to develop an end-to-end model that removes separate steps. Kikegawa et al. (2003) worked on a simplified physics-based heat transfer algorithms that were integrated into urban canopy models to account for interactions between buildings and their surrounding environment. Umi, a Rhinoceros-based urban energy-modeling tool that allows users to carry out energy, daylighting, and walkability assessment of the neighborhood is another coupling model (Reinhart et al. 2013). However, many of such models are either computationally inefficient or limited in the areas explored due to the complexity of physics simulation, and an alternative surrogate model is needed. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- *Review of related problems*\n",
    "\n",
    "  The neural network, especially Deep Learning has gained prominence as a prediction model that could replace a physics model. Many researchers including Singaravel et al, (2018) explored the potential application of Long-Short-Term-Memory (LSTM) architecture in monthly energy consumption prediction and the use of Transfer Learning to greatly reduce the complexity of learning a model. However, to the best of our knowledge, not much deep learning applied research has been done in the field of BPS.\n",
    "\t\n",
    "  In a review done by Niu et al. (2017),  various data-driven predictive models were compared and assessed based on their ability to accurately forecast solar irradiation using hourly weather data. Among the models that were tested, which included Artificial Neural Network (ANN), Support Vector Machine (SVM), State Space (SS) and a Bayesian Network (BM). It was found that the most successful model was the RNN model, which had the greatest prediction accuracy, over the prediction period, which spanned a total of 3 months. However, this particular study utilized a small set of input variables and also found that a decreased sampling rate resulted in decreased performance of the model, although computational speed was increased. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- *Review of 3D problem*\n",
    "\t\n",
    "  RNNs have been proven to excel at modelling sequential data, by giving them access to to previous context and time-dependencies in data. However, a current limitation of the RNN structure is the fact that the input is explicitly required to be single-dimensional, which requires any multi-dimensional data to be pre-processed and flattened before being fed into the RNN model architecture.] CNNs are an example of NN that are able to use multidimensional data (for images). However, CNNs lose the ability to learn from long-term memory, as well as the large increase in computational cost as the input data increases due to the multi-dimensionality. This holds true In general, by increasing the dimensionality of data/architecture of NN models, the computational time increases greatly, due to the multitude of layers and parameters involved in the tuning of such architectures. \n",
    "\n",
    "  One proposed method in literature to extend the functionality of RNNs to multi-dimensional data is discussed by Graves et al, (2007). The proposed method extends the dimensionality of the data input into the RNN architecture, while avoiding the extreme scaling issues experienced by CNNs. The Multi-Dimensional Recurrent Neural Network (MDNN)  involves altering the architecture of the  RNN to expand the number of recurrent connections and forget gates such that there is one for each dimension. This may be an interesting architecture to look into for our specific problem, which is multidimensional - both spatially and temporally (3-dimensional buildings, with values changing over time).\n",
    "\n",
    "  Another possible approach for our specific problem involving surfaces of various buildings, and the prediction of radiation values, one possible approach is to slice the 3-dimensional building representation into 2-dimensional segments (similar to the CT scan discussed below), and feed each 2-dimensional slice into a CNN. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Research with a similar data structure\n",
    "\n",
    "  Though there are not many studies applying deep learning in the field of BPS, there are other studies that have a similar data structure that could be a good reference for our model.\n",
    "\n",
    "  One study is the analysis of MRI images, for example, “3D Convolutional Neural Networks for Tumor Segmentation using Long-range 2D Context“ (Mlynarski et al., 2018). MRI is a scanning technology that can generate images of slices of the imaging target. With 2D images stacking as a pile, it is actually 3D information. However, segmentation of tumors in large medical images is still a very challenging task. One of the main drawbacks of CNNs is their computational cost resulting from applications of thousands of costly operations (convolutions, poolings, upsampling) on input images. This aspect is particularly problematic for segmentation problems in large medical images such as MRI or CT scans. The authors proposed a neural network that can perform tumor segmentation based on the 2D-3D images. The model extracts features by 2D CNNs from a long-range 2D context in three orthogonal directions, and the features are used as an additional input to a 3D CNN. Such design considerably increases the size of the receptive field compared to standard 3D models taking as input only the raw intensities of voxels of a subvolume.\n",
    "\n",
    "  Another study that may be a great reference is the analysis of point cloud data in the field of Robotics, for example, “VoxNet: A 3D Convolutional Neural Network for Real-Time Object Recognition” (Maturana et al., 2015). Point cloud data is a set of point with coordinates in 3D space, measured by LiDAR or RGBD camera. The authors proposed the VoxNet, a basic 3D CNN architecture that can be applied to create fast and accurate object class detectors for 3D point cloud data. VoxNet is composed of an input layer, two 3D convolutional layers, a maxpool layer, a fully connected network, and the output layer. In order to cover objects of different scales, e.g. a truck or a traffic sign, a multiresolution VoxNet can be achieved by combining two networks with an identical VoxNet architectures, each receiving occupancy grids at different resolutions, and fuse the information from both networks by concatenating the outputs of their respective FC(128) layers. With other preprocessing and training techniques, VoxNet achieves a surprisingly good result with such a simple structure. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  For our proposed problem, there is some similarity in our data representation to the 3D data structure of the studies mentioned above. However, the nature of our problem, deep learning methods to represent physical properties on buildings (annual radiation intensity and exposure), is quite different from those. In the problem, a tall building may have a significant effect (causing low radiation) on the low building in its shadow depending on the direction of sunlight, and the voxel of these two building may be far away in the spatial relationship. The best structure to embed spatial information is the convolution layers, and we may need a deep network to embed information that covers a wide range as described. However, 3D convolution is computationally costly, and a deep network may not be applicable. In addition, the objective of the above-mentioned research with a similar data structure is categorical, while the output of our model will be numerical (radiation), which may be more difficult in some sense. These challenges may be overcome by applying techniques in the literature to interpret large spatial information wisely. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[*Figures*]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IrCAUQ6nKhvF"
   },
   "outputs": [],
   "source": [
    "import sys, os, glob\n",
    "import calendar\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from IPython.display import clear_output\n",
    "import scipy.io\n",
    "pd.set_option('display.max_colwidth', -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MATLAB Preprocessing Script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```matlab\n",
    "% matlab codes here\n",
    "\n",
    "% Somehow there is no color highlighting for matlab keyword. \n",
    "% You can use `publish(script_name,'evalCode',false);` to generate a html from MATLAB code. If you comment the script properly (e.g. using sections (%%), etc), the result look pretty good. I tried to copy-paste the html to the notebook cell, while the format is correct but still not showing color. \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TRANSFORM MAT FILE TO NUMPY FOR MODEL INPUT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GET ALL FILES THAT ENDS WITH .MAT EXTENSION\n",
    "FILEPATH = 'preprocessed_output_combined_with_boundary'\n",
    "files = [file for file in glob.glob(FILEPATH + \"/*.mat\")]\n",
    "print (\"NUMBER OF FILES IN RESULT FOLDER: \", len(files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST\n",
    "mat = scipy.io.loadmat(files[0])\n",
    "X = mat['X_input']\n",
    "Y = mat['y_rad']\n",
    "print (X.shape)\n",
    "print (Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TAKES ABOUT 15 SECONDS.\n",
    "X, Y = [], []\n",
    "cnt, total = 0, len(files)\n",
    "for item in files:\n",
    "    cnt += 1\n",
    "    print (\"PROGRESS: \", round(cnt / total * 100, 2), \"%\")\n",
    "    clear_output(wait=True)\n",
    "    mat = scipy.io.loadmat(item)\n",
    "    X.append(mat['X_input'])\n",
    "    Y.append(mat['y_rad'])\n",
    "\n",
    "X = np.array(X)\n",
    "Y = np.array(Y)\n",
    "\n",
    "print (X.shape)\n",
    "print (Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SANITY CHECKS\n",
    "print (\"[Input X]\\nMin:\" , X.min(), \"Max:\", X.max())\n",
    "print ()\n",
    "print (\"[Input y]\\nMin:\" , Y.min(), \"Max:\", Y.max())\n",
    "print ()\n",
    "print ('INPUT UNIQUE (SHOULD BE [0,1]): ', np.unique(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_norm = 1500\n",
    "Y = Y/Y_norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization of Processed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forceAxisEqual(ax, X, Y, Z):\n",
    "    max_range = np.array([X.max()-X.min(), Y.max()-Y.min(), Z.max()-Z.min()]).max() / 2.0\n",
    "\n",
    "    mid_x = (X.max()+X.min()) * 0.5\n",
    "    mid_y = (Y.max()+Y.min()) * 0.5\n",
    "    mid_z = (Z.max()+Z.min()) * 0.5\n",
    "    ax.set_xlim(mid_x - max_range, mid_x + max_range)\n",
    "    ax.set_ylim(mid_y - max_range, mid_y + max_range)\n",
    "    ax.set_zlim(0, max_range*2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotBuilding(x,y):\n",
    "    building = []\n",
    "    radiation = []\n",
    "    color = {0: 'blue', 1:'red'}\n",
    "\n",
    "    boundary = scipy.io.loadmat('combined_boundary.mat')\n",
    "    boundary = boundary['boundary_building_mat']\n",
    "\n",
    "    for i in range(0, 51):\n",
    "        for j in range(0, 51):\n",
    "            for k in range(0, 51):\n",
    "                if x[i][j][k] > 0:\n",
    "                    building.append([i,j,k,boundary[i][j][k]])\n",
    "                    if y[i,j,k]!=0:\n",
    "                        radiation.append([i,j,k,y[i][j][k]])\n",
    "\n",
    "    dfBuilding = pd.DataFrame(data=building)  \n",
    "    dfBuilding[3] = dfBuilding[3].apply(lambda x: color[int(x)])\n",
    "    dfRadiation = pd.DataFrame(data=radiation)  \n",
    "\n",
    "    fig = plt.figure(figsize=(20,10))\n",
    "    ax0 = fig.add_subplot(121, projection='3d')\n",
    "    ax0.scatter(dfBuilding[0], dfBuilding[1], dfBuilding[2], s=2, color=dfBuilding[3])\n",
    "    ax0.set_xlabel('X axis')\n",
    "    ax0.set_ylabel('Y axis')\n",
    "    ax0.set_zlabel('Z axis')\n",
    "    forceAxisEqual(ax0,dfBuilding[0], dfBuilding[1], dfBuilding[2])\n",
    "    ax0.set_title('Building Architecture (blue for target building, red for surronding building)')\n",
    "    \n",
    "    ax1 = fig.add_subplot(122, projection='3d')\n",
    "    im = ax1.scatter(dfRadiation[0], dfRadiation[1], dfRadiation[2], \n",
    "                     s=20, c=dfRadiation[3], cmap='hot', edgecolors='grey',vmin=0, vmax=1)\n",
    "    ax1.set_xlabel('X axis')\n",
    "    ax1.set_ylabel('Y axis')\n",
    "    ax1.set_zlabel('Z axis')\n",
    "    forceAxisEqual(ax1,dfRadiation[0], dfRadiation[1], dfRadiation[2])\n",
    "    ax1.set_title('Radiation Intensity (normalized)')\n",
    "    plt.colorbar(im)\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt_idx = files.index('preprocessed_output_combined_with_boundary/9,27,21.mat') # np.random.randint(len(X)) # CHANGE THIS TO SEE WHICH FILE YOU WANT TO VISUALIZE\n",
    "plotBuilding(X[plt_idx], Y[plt_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.expand_dims(X, axis=-1)\n",
    "Y = np.expand_dims(Y, axis=-1)\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=99)\n",
    "print(X_train.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EcFuvahzv54D"
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras import backend as K\n",
    "from keras.models import Model\n",
    "from keras.layers import Flatten, Dense, Input, Reshape, Lambda, Concatenate\n",
    "from keras.layers import Conv2D, MaxPooling2D, UpSampling2D, ZeroPadding2D, Permute, Cropping2D\n",
    "from keras.layers import Conv3D, Deconvolution3D, MaxPooling3D, UpSampling3D, ZeroPadding3D, Cropping3D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Function\n",
    "\n",
    "The goal of the project is to predict the radiation intensity on buildins. Thus, the output of the network is a numerical value and we would like to use the MSE as the loss function.\n",
    "However, the simulation to generate the dataset can only get the radiation intensity at the surface of buildings. We then define a customized loss function to only account for the MSE loss at the surface of buildings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RadiationLoss(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Compute the loss for the radiation matrix.\n",
    "    \n",
    "    Inputs:\n",
    "    - y_true: radiation of the target building. 3D Tensor with radiation value at taget surface and others 0.\n",
    "    - y_pred: the prediction of the radiation.\n",
    "    \n",
    "    Returns:\n",
    "    - scalar mse loss, only calculated where radiation value not equal to zero\n",
    "    \"\"\"\n",
    "    \n",
    "    y_loc = K.cast(K.not_equal(y_true,K.constant(0)),'float')\n",
    "    return K.sum(K.pow(y_true-y_pred*y_loc,2))/K.sum(y_loc)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Architecture\n",
    "\n",
    "  When we build the neural network, we refer to the literature, *VoxNet: A 3D Convolutional Neural Network for Real-Time Object Recognition* (Maturana et al., 2015). **Voxnet** is 3D CNN that can be applied to create fast and accurate object class detectors for 3D point cloud data. We value the literature because the architecture of Voxnet is simple, and our dataset is also similar to point cloud (0/1 in the space).\n",
    "  In order to increase the range of captured information, i.e. to handle the shadow issue, we increase the depth of Voxnet. We also apply an autoencoder structure to map the latent variables back to 3D space, where our radiation results lie in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_size = (51, 51, 51, 1)\n",
    "\n",
    "inp = Input(matrix_size)\n",
    "\n",
    "# Voxnet structure + autoencoder\n",
    "enc = Conv3D(32, kernel_size=5, strides=2, padding='same', activation='relu')(inp)\n",
    "enc = Conv3D(32, kernel_size=3, strides=1, padding='same', activation='relu')(enc)\n",
    "enc = MaxPooling3D((2,2,2))(enc)\n",
    "\n",
    "enc = Conv3D(64, kernel_size=5, strides=2, padding='same', activation='relu')(enc)\n",
    "enc = Conv3D(64, kernel_size=3, strides=1, padding='same', activation='relu')(enc)\n",
    "enc = MaxPooling3D((2,2,2))(enc)\n",
    "conv_shape = enc.get_shape().as_list()\n",
    "\n",
    "enc = Flatten()(enc)\n",
    "latent = Dense(256, activation='relu')(enc)\n",
    "\n",
    "dec = Dense(np.prod(conv_shape[1:]), activation='relu')(latent)\n",
    "dec = Reshape(conv_shape[1:])(dec)\n",
    "\n",
    "dec = UpSampling3D((2,2,2))(dec)\n",
    "dec = Deconvolution3D(64, kernel_size=3, strides=1, padding='same', activation='relu')(dec)\n",
    "dec = Deconvolution3D(64, kernel_size=5, strides=2, padding='same', activation='relu')(dec)\n",
    "\n",
    "dec = UpSampling3D((2,2,2))(dec)\n",
    "dec = Deconvolution3D(32, kernel_size=3, strides=1, padding='valid', activation='relu')(dec)\n",
    "dec = Deconvolution3D(32, kernel_size=5, strides=2, padding='same', activation='relu')(dec)\n",
    "dec = Cropping3D(((0,1),(0,1),(0,1)))(dec)\n",
    "\n",
    "out = Conv3D(1, kernel_size=3, strides=1, padding='same', activation='sigmoid')(dec) # Assume normalized data [0,1]\n",
    "\n",
    "voxnet_model = Model(inp, out)\n",
    "voxnet_model.compile(optimizer='adam',loss=RadiationLoss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "voxnet_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = voxnet_model.fit(X_train, Y_train, epochs=10, validation_data=(X_test, Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "voxnet_model.save_weights('voxnet.w')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Result\n",
    "\n",
    "We can see that both training loss and validation loss decrease to around 3e-4 after 10 epochs, i.e. ~2% error on average. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Val'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotRadiation(y_true,y_pred):\n",
    "    r_true = []\n",
    "    r_pred = []\n",
    "\n",
    "    for i in range(0, 51):\n",
    "        for j in range(0, 51):\n",
    "            for k in range(0, 51):\n",
    "                if y_true[i][j][k] != 0:\n",
    "                    r_true.append([i,j,k,y_true[i][j][k]])\n",
    "                    r_pred.append([i,j,k,y_pred[i][j][k]])\n",
    "\n",
    "    dfTrue = pd.DataFrame(data=r_true)  \n",
    "    dfPred = pd.DataFrame(data=r_pred)  \n",
    "\n",
    "    fig = plt.figure(figsize=(20,5))\n",
    "    ax0 = fig.add_subplot(131, projection='3d')\n",
    "    im = ax0.scatter(dfTrue[0], dfTrue[1], dfTrue[2], \n",
    "                     s=20, c=dfTrue[3], cmap='hot', edgecolors='grey',vmin=0, vmax=1)\n",
    "    ax0.set_xlabel('X axis')\n",
    "    ax0.set_ylabel('Y axis')\n",
    "    ax0.set_zlabel('Z axis')\n",
    "    forceAxisEqual(ax0,dfTrue[0], dfTrue[1], dfTrue[2])\n",
    "    ax0.set_title('y_true')\n",
    "    plt.colorbar(im)\n",
    "    \n",
    "    ax1 = fig.add_subplot(132, projection='3d')\n",
    "    im = ax1.scatter(dfPred[0], dfPred[1], dfPred[2], \n",
    "                     s=20, c=dfPred[3], cmap='hot', edgecolors='grey',vmin=0, vmax=1)\n",
    "    ax1.set_xlabel('X axis')\n",
    "    ax1.set_ylabel('Y axis')\n",
    "    ax1.set_zlabel('Z axis')\n",
    "    forceAxisEqual(ax1,dfPred[0], dfPred[1], dfPred[2])\n",
    "    ax1.set_title('y_pred')\n",
    "    plt.colorbar(im)\n",
    "    \n",
    "    ax2 = fig.add_subplot(133, projection='3d')\n",
    "    error = dfPred[3]-dfTrue[3]\n",
    "    e_range = np.max(abs(error))\n",
    "    im = ax2.scatter(dfPred[0], dfPred[1], dfPred[2], \n",
    "                     s=20, c=error, cmap='RdBu', edgecolors='grey',vmin=-e_range, vmax=e_range)\n",
    "    ax2.set_xlabel('X axis')\n",
    "    ax2.set_ylabel('Y axis')\n",
    "    ax2.set_zlabel('Z axis')\n",
    "    forceAxisEqual(ax2,dfPred[0], dfPred[1], dfPred[2])\n",
    "    ax2.set_title('error (y_pred-y_true)')\n",
    "    plt.colorbar(im)\n",
    "    \n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt_idx = [73, 127, 172,  16, 145] # np.random.randint(len(Y_test), size=5)\n",
    "for i in plt_idx:\n",
    "    plotRadiation(Y_test[i], voxnet_model.predict(np.expand_dims(X_test[i],axis=0))[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that when we visualize each building to have a closer look at the prediction, we found that there is high error in some data at the boundary between the three buildings, even when the neibouring buildings are at the same height. We can then tell that the neural network learns some specific rules about the boundary between the buildings based on the training data, but not the actual physics about the radiation intensity. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alternate Model\n",
    "\n",
    "We also look into the literature, *3D Convolutional Neural Networks for Tumor Segmentation using Long-range 2D Context* (Mlynarski et al., 2018). The model have the benefit of extracting long-range information using 2D model and combining them in a 3D model. We refer the literature and build a corresponding model, but the model have ~13M parameters with lots of submodel, which cause JupyterHub ResourceExhaustedError."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractChannel(inp, i):\n",
    "    # Input 2D image with K channel. extract i-th channel\n",
    "    return K.expand_dims(inp[:,:, i])\n",
    "\n",
    "def sqeezeChannel(inp):\n",
    "    return K.squeeze(inp,axis=-1)\n",
    "\n",
    "def expandChannel(inp):\n",
    "    return K.expand_dims(inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_size = (51, 51, 51, 1)\n",
    "last_channel = 30\n",
    "\n",
    "# 2D network\n",
    "def sub2Dnetwork_1c():\n",
    "    inp_2D = Input((matrix_size[0], matrix_size[1], 1))\n",
    "\n",
    "    x = Conv2D(32, padding='same', kernel_size=3)(inp_2D)\n",
    "    x = Conv2D(32, padding='same', kernel_size=3)(x)\n",
    "    x = MaxPooling2D((4,4))(x)\n",
    "    x = Conv2D(64, padding='same', kernel_size=3)(x)\n",
    "    x = Conv2D(32, padding='same', kernel_size=3)(x)\n",
    "    x = UpSampling2D((4,4))(x)\n",
    "    x = ZeroPadding2D(((1,2),(1,2)))(x)\n",
    "    x = Conv2D(32, padding='same', kernel_size=3)(x)\n",
    "    out = Conv2D(last_channel, padding='same', kernel_size=3)(x)\n",
    "\n",
    "    return  Model(inp_2D, out)\n",
    "\n",
    "def sub2Dnetwork_51c():\n",
    "    inp_2D = Input((matrix_size[:-1]))\n",
    "\n",
    "    x = Conv2D(32, padding='same', kernel_size=3)(inp_2D)\n",
    "    x = Conv2D(32, padding='same', kernel_size=3)(x)\n",
    "    x = MaxPooling2D((4,4))(x)\n",
    "    x = Conv2D(64, padding='same', kernel_size=3)(x)\n",
    "    x = Conv2D(32, padding='same', kernel_size=3)(x)\n",
    "    x = UpSampling2D((4,4))(x)\n",
    "    x = ZeroPadding2D(((1,2),(1,2)))(x)\n",
    "    x = Conv2D(32, padding='same', kernel_size=3)(x)\n",
    "    out = Conv2D(last_channel, padding='same', kernel_size=3)(x)\n",
    "\n",
    "    return  Model(inp_2D, out)\n",
    "\n",
    "def combine2Dnetwork():\n",
    "    inp_2D = Input((matrix_size[0], matrix_size[1], last_channel*(matrix_size[2]+1)))\n",
    "    x = Conv2D(32, padding='same', kernel_size=3)(inp_2D)\n",
    "    x = MaxPooling2D((2,2))(x)\n",
    "    x = Conv2D(64, padding='same', kernel_size=3)(x)\n",
    "    x = MaxPooling2D((2,2))(x)\n",
    "    x = Conv2D(128, padding='same', kernel_size=3)(x)\n",
    "    x = UpSampling2D((2,2))(x)\n",
    "    x = Conv2D(64, padding='same', kernel_size=3)(x)\n",
    "    x = UpSampling2D((2,2))(x)\n",
    "    x = ZeroPadding2D(((1,2),(1,2)))(x)\n",
    "    x = Conv2D(32, padding='same', kernel_size=3)(x)\n",
    "    out = Conv2D(matrix_size[2], padding='same', kernel_size=3)(x)\n",
    "\n",
    "    return Model(inp_2D, out)\n",
    "\n",
    "def get2Dnetwork():\n",
    "    inp = Input(matrix_size[:-1])\n",
    "    subnetworks = []\n",
    "    suboutputs = []\n",
    "    for i in range(matrix_size[-2]):\n",
    "        subnet = sub2Dnetwork_1c()\n",
    "        subout = subnet(Lambda(lambda x: extractChannel(x, i))(inp))\n",
    "        suboutputs.append(subout)\n",
    "        subnetworks.append(subnet)\n",
    "    subnet = sub2Dnetwork_51c()\n",
    "    subout = subnet(inp)\n",
    "    suboutputs.append(subout)\n",
    "    subnetworks.append(subnet)\n",
    "\n",
    "    inp_combined = Concatenate()(suboutputs)\n",
    "    comb = combine2Dnetwork()\n",
    "    return Model(inp,comb(inp_combined))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3D network\n",
    "inp = Input(matrix_size)\n",
    "inp_2D = Lambda(sqeezeChannel)(inp)\n",
    "\n",
    "X_model = get2Dnetwork()\n",
    "Y_model = get2Dnetwork()\n",
    "Z_model = get2Dnetwork()\n",
    "\n",
    "inp_2D_X = Permute((1,2,3))(inp_2D)\n",
    "inp_2D_Y = Permute((1,3,2))(inp_2D)\n",
    "inp_2D_Z = Permute((2,3,1))(inp_2D)\n",
    "\n",
    "out_2D_X = X_model(inp_2D_X)\n",
    "out_2D_Y = Y_model(inp_2D_Y)\n",
    "out_2D_Z = Z_model(inp_2D_Z)\n",
    "\n",
    "inp_3D = Concatenate()([inp,\n",
    "                        Lambda(expandChannel)(out_2D_X),\n",
    "                        Lambda(expandChannel)(out_2D_Y),\n",
    "                        Lambda(expandChannel)(out_2D_Z)])\n",
    "\n",
    "enc = Conv3D(32, kernel_size=3, strides=1, padding='same', activation='relu')(inp_3D)\n",
    "enc = Conv3D(32, kernel_size=3, strides=1, padding='same', activation='relu')(enc)\n",
    "enc = MaxPooling3D((2,2,2))(enc)\n",
    "\n",
    "enc = Conv3D(64, kernel_size=3, strides=1, padding='same', activation='relu')(enc)\n",
    "enc = Conv3D(64, kernel_size=3, strides=1, padding='same', activation='relu')(enc)\n",
    "enc = MaxPooling3D((2,2,2))(enc)\n",
    "\n",
    "enc = Conv3D(128, kernel_size=3, strides=1, padding='same', activation='relu')(enc)\n",
    "enc = Conv3D(128, kernel_size=3, strides=1, padding='same', activation='relu')(enc)\n",
    "dec = UpSampling3D((2,2,2))(enc)\n",
    "\n",
    "dec = Conv3D(64, kernel_size=3, strides=1, padding='same', activation='relu')(dec)\n",
    "dec = Conv3D(64, kernel_size=3, strides=1, padding='same', activation='relu')(dec)\n",
    "dec = UpSampling3D((2,2,2))(dec)\n",
    "dec = ZeroPadding3D(((1,2),(1,2),(1,2)))(dec) # pad 0s at one side to match the size\n",
    "\n",
    "dec = Conv3D(32, kernel_size=3, strides=1, padding='same', activation='relu')(dec)\n",
    "dec = Conv3D(32, kernel_size=3, strides=1, padding='same', activation='relu')(dec)\n",
    "\n",
    "out = Conv3D(1, kernel_size=3, strides=1, padding='same', activation='sigmoid')(dec) # Assume normalized data [0,1]\n",
    "\n",
    "lr2D_model = Model(inp, out)\n",
    "lr2D_model.compile(optimizer='adam',loss=RadiationLoss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr2D_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = lr2D_model.fit(X_train, Y_train, epochs=10, validation_data=(X_test, Y_test)) # ResourceExhaustedError"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "main.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
